# -*- coding: utf-8 -*-
"""
æƒé™è¯†åˆ«æ•ˆæœè¯„æµ‹è„šæœ¬

è¾“å…¥ï¼š
  processed/
    fastbot-xxx/
      goal_labels.json
      results_permission_rule_only.json
      results_permission_llm_only.json
      results_permission_rule_llm.json

åŠŸèƒ½ï¼š
  - å¿½ç•¥ true_permissions ä¸ºç©ºçš„é“¾æ¡
  - å¯¹ä¸‰ç§æ–¹æ³•åˆ†åˆ«è®¡ç®—ï¼š
        Accuracy
        Precision(micro)
        Recall(micro)
        F1(micro)
        FP / FN / TP
        æ¯ä¸ªæƒé™çš„è¯¦ç»†è¡¨ç°
  - è¾“å‡ºæ±‡æ€»ä¿¡æ¯
"""

import os
import json
from collections import defaultdict, Counter

EVAL_FILES = {
    "rule_only": "results_permission_rule_only.json",
    "llm_only": "results_permission_llm_only.json",
    "rule_llm": "results_permission_rule_llm.json",
}


def load_json(path):
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def eval_one_method(gt_labels, pred_results):
    """
    gt_labels: dict(chain_id â†’ true_permissions)
    pred_results: list({chain_id, predicted_permissions})

    è¿”å› metrics å­—å…¸
    """
    pred_map = {x["chain_id"]: x["predicted_permissions"] for x in pred_results}

    TP = 0
    FP = 0
    FN = 0

    per_perm = Counter()     # è®°å½•æ¯ä¸ªæƒé™å‡ºç°æ¬¡æ•°
    per_perm_TP = Counter()
    per_perm_FP = Counter()
    per_perm_FN = Counter()

    total = 0  # å‚ä¸è¯„æµ‹çš„é“¾æ¡æ•°ï¼ˆæ’é™¤ true_permissions ä¸ºç©ºçš„ï¼‰

    for cid, gt_perms in gt_labels.items():

        # è¿‡æ»¤æ‰ true_permissions == [] çš„é“¾æ¡
        if not gt_perms:
            continue

        total += 1
        pred = pred_map.get(cid, [])

        gt_set = set(gt_perms)
        pred_set = set(pred)

        # ç»Ÿè®¡æ¯ä¸ªæƒé™å‡ºç°æ¬¡æ•°
        for p in gt_set:
            per_perm[p] += 1

        # è®¡ç®— TP / FP / FN
        tp_set = gt_set & pred_set
        fp_set = pred_set - gt_set
        fn_set = gt_set - pred_set

        TP += len(tp_set)
        FP += len(fp_set)
        FN += len(fn_set)

        # æ¯ä¸ªæƒé™çº§åˆ«
        for p in tp_set:
            per_perm_TP[p] += 1
        for p in fp_set:
            per_perm_FP[p] += 1
        for p in fn_set:
            per_perm_FN[p] += 1

    # è®¡ç®—æŒ‡æ ‡
    accuracy = TP / (TP + FP + FN + 1e-6)
    precision = TP / (TP + FP + 1e-6)
    recall = TP / (TP + FN + 1e-6)
    f1 = 2 * precision * recall / (precision + recall + 1e-6)

    return {
        "total_evaluated_chains": total,
        "TP": TP,
        "FP": FP,
        "FN": FN,
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "per_permission_stats": {
            p: {
                "gt_count": per_perm[p],
                "TP": per_perm_TP[p],
                "FP": per_perm_FP[p],
                "FN": per_perm_FN[p],
                "precision": per_perm_TP[p] / (per_perm_TP[p] + per_perm_FP[p] + 1e-6),
                "recall": per_perm_TP[p] / (per_perm_TP[p] + per_perm_FN[p] + 1e-6),
            }
            for p in per_perm.keys()
        }
    }


def main(processed_root):
    all_apps = [
        d for d in os.listdir(processed_root)
        if d.startswith("fastbot-")
    ]

    print(f"\nğŸ“‚ æ‰¾åˆ° {len(all_apps)} ä¸ª app\n")

    # æ±‡æ€»æ¯ç§æ–¹æ³•çš„æŒ‡æ ‡
    results_summary = {
        "rule_only": [],
        "llm_only": [],
        "rule_llm": [],
    }

    for app in all_apps:
        app_dir = os.path.join(processed_root, app)

        gt = load_json(os.path.join(app_dir, "goal_labels.json"))
        if gt is None:
            print(f"âš  è·³è¿‡ï¼ˆæ— çœŸå®æ ‡ç­¾ï¼‰ï¼š{app}")
            continue

        # æ„å»º gt dict
        gt_map = {item["chain_id"]: item["true_permissions"] for item in gt}

        print(f"\n==============================")
        print(f"ğŸ“Œ è¯„æµ‹ APPï¼š{app}")

        for method, filename in EVAL_FILES.items():
            pred = load_json(os.path.join(app_dir, filename))
            if pred is None:
                print(f"  âš  {method} æ— ç»“æœï¼Œè·³è¿‡")
                continue

            metrics = eval_one_method(gt_map, pred)
            results_summary[method].append(metrics)

            print(f"\nğŸ” æ–¹æ³•ï¼š{method}")
            print(f"   å‚ä¸è¯„æµ‹é“¾æ¡æ•°ï¼š{metrics['total_evaluated_chains']}")
            print(f"   TP={metrics['TP']}  FP={metrics['FP']}  FN={metrics['FN']}")
            print(f"   Accuracy = {metrics['accuracy']:.4f}")
            print(f"   Precision = {metrics['precision']:.4f}")
            print(f"   Recall = {metrics['recall']:.4f}")
            print(f"   F1 = {metrics['f1']:.4f}")

    # ======== è¾“å‡ºæ•´ä½“å¹³å‡ç»“æœ ========
    print("\n\n==============================")
    print("ğŸ“Š **æœ€ç»ˆæ•´ä½“ç»“æœï¼ˆå¹³å‡ over all appsï¼‰**")
    print("==============================\n")

    for method, lst in results_summary.items():
        if not lst:
            continue

        avg = {
            "accuracy": sum(x["accuracy"] for x in lst) / len(lst),
            "precision": sum(x["precision"] for x in lst) / len(lst),
            "recall": sum(x["recall"] for x in lst) / len(lst),
            "f1": sum(x["f1"] for x in lst) / len(lst),
        }

        print(f"\nâ­ æ–¹æ³•ï¼š{method}")
        print(f"   Avg Accuracy  = {avg['accuracy']:.4f}")
        print(f"   Avg Precision = {avg['precision']:.4f}")
        print(f"   Avg Recall    = {avg['recall']:.4f}")
        print(f"   Avg F1        = {avg['f1']:.4f}")


if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼š python evaluate_permissions.py <processed_dir>")
        sys.exit(1)

    processed_root = sys.argv[1]
    main(processed_root)